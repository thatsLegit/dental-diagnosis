# -*- coding: utf-8 -*-
"""Copie de CI - Data Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iCVhXPC38dHdu85tnV8tTULSgey-DjeA

# Cover

**Title:** Machine Learning - Dataset 1 - Periodontal Diagnosis

**Subject:** Computational Intelligence

**Professors:** Umberto Grandi & Sylvain Cussat-Blanc

**Writers:** 

*   Chaudhari, Nimish - 21912044
*   Quintero, Reynaldo - 21912040
*   Stepanov, Ilya - 21400267
*   Titis, Jorgo - 21911054

**Location:** 
31200 Toulouse, France. Université Toulouse I Capitole. 

**Year of creation**: 2020.

# Abstract

The purpose of this project was to apply different techniques of machine learning on a realworld dataset. The dataset studied contained different features of patients from the Centre Hopitalier Universitaire
(CHU) of Toulouse, and had the purpose of defining if the patients were at risk of having a disease on their tooth supporting tissues. The machine learning techniques applied were: SVM, KNN and Random Forest. After preprocessing the data and tuning the models to obtain the best results for the available data, the most accurate algorithm was a linear SVM model, with a 100% accuracy.

# Sections



*   [Introduction](#scrollTo=JL7m8cWbUEVu)
*   [First approach to dataset](#scrollTo=YR6EWh1rIeVb)
*   [Functions used](#scrollTo=DplEqa4cUySI)
*   [Data visualization and treatment](#scrollTo=491MvOwxTk73)
*   [Creating, tuning and training the models](#scrollTo=G6bm5OH0U94w)
*  [Prediction of missing data](#scrollTo=KU-SEVpuJRB7)
*  [Conclusion](#scrollTo=mnId5RW9pCLV)

# Introduction


The purpose of this project was to apply different techniques of machine learning on realworld datasets. 

The dataset studied contained different features of patients from the Centre Hopitalier Universitaire
(CHU) of Toulouse.

Specifically, the dataset provided information of different patient features that allowed, according to the experts who created the dataset, to predict the diagnosis of said patients regarding their tooth supporting tissues.

## General objective:

Define the probability for a patient to have a disease on their tooth supporting tissues from diferent features of said patient that can be acquired remotely.

## Specific objectives:

*   Perform a detailed analysis on the dataset.
*   Perform data preprocessing techniques to increase data quality.
*   Create, tune and train 3 different preselected machine learning models that are able to predict the target feature from the dataset.
*   Decide on the best created model, based on defined indicators.
*   Use the decided best model to perform a prediction on the missing diagnosis in the dataset.

## Scope
The scope of this project is confined to the provided dataset, as it is the only information that was available. This means that the dataset will be used both for training and for testing.

Due to time constraint, only some techniques of data preprocessing will be used, as well as only some parameters will be changed to train the best model possible.

# First approach to the dataset

The first step taken to start this project was the visualization of the given dataset.

Said dataset was given in a xlsx format and consisted in three sheets:


1.   **Definition:** a sheet containing a description of the general objective expected to be obtained from the dataset, as well as specifying the feature to be predicted and its possible values.
2.   **Lexic:** a sheet containing the different variables of the dataset and the explanation of each one. The exact list is shown below.

>1.   **ID:** ID of the subject.
>2.   **Sex**
>3.   **Age**
>4.   **BMI:** Body Mass Index.
>5.   **Smoking:** Smoking consumption.
>6.   **Pathologies:** Systemic diseases.
>7.   **Pregnant:** Yes/No.
>8.   **Food_Sugar:** Consumption of sweet foods.
>9.   **Fat_Salty:** Consumption of fatty or salty foods.
>10.   **Soda:** Consumption of soda.
>11.   **Alcohol:** Consumption of alcohol.
>12.   **Frequence_Appoint_Dentist:** Frequency of Dental Surgeon Appointments.
>13.   **Hygiene_Dental:** Dental hygiene (number of brushing teeth a day).
>14.   **Gingivorrhagia:** Gingival bleeding.
>15.   **Stress_Daily:** Is the patient stressed.
>16.   **PI:** Dental plaque quantity.
>17.   **Diagnosis:** Periodontal diagnosis (to predict).

3.   **EpiParo:** a sheet containing the previously described dataset.

In a first inspection of the dataset, it was possible to determine the data type of each of the columns. This was done using the filtering function in Excel, which shows all the values appearing in the dataset. These are detailed below.

0. **ID:** an autoincreasing number.
2. **Sex**: a string value: 'Woman' or 'Man'
3. **Age**: an integer representing the age of the patient
4. **BMI:** a float containing the calculated BMI.
4. **Smoking**: a string that represents a category. The possible categories within the dataset were: 'No smoker', 'Previous smoker' or 'Smoker'.
5. **Pathologies:** a string representing the answer: 'Yes', 'No' or 'N/A' 
6. **Pregnant:** a string representing the answer: 'Yes', 'No' or 'N/A'. 
7. **Food_Sugar:** a string representing the answer: 'Never', 'Sometimes', 'Several times a week', 'Once a day' or 'Several times a day'.
8. **Fat_Salty:** a string representing the answer: 'Never', 'Sometimes', 'Several times a week', 'Once a day' or 'Several times a day'.
9. **Soda:** a string representing the answer: 'Never', 'Sometimes', 'Several times a week', 'Once a day' or 'Several times a day'.
10. **Alcohol:** a string representing the answer: 'Never', 'Sometimes', 'Several times a week', 'Once a day' or 'Several times a day'.
11. **Frequence_Appoint_Dentist:** a string representing the answer: 'Never', 'Once a year', '2-3 a year' or 'Regularly'. There were empty blocks.
12. **Hygiene:** integer representing the number of times a day a patient brushed their teeth (0,1,2,3,4,5,6).
13. **Gingivorrhagia:** a string representing the answer: 'Absent', 'Corrélées (cycles, etc...)', 'Provoked' or 'Spontanées'. There were empty blocks.
14. **Stress_Daily:** an integer representing the level of stress of the person (1-10). There were empty blocks.
15. **PI:** an integer representing the level of quantity of dental plaque (0-3). There were empty blocks.
16. **Diagnosis:** - a string representing the answer: 'Healthy', 'Periodontitis' or 'Gingivitis'. There were empty blocks.

Firstly, after reviewing the data types within of the dataset, it was clear that the problem faced was a classification problem.

Secondly, a decision had to be made on: the way to transform the string values into number values so that they could be later used to train the models; and what to do with the null values that were present.

For the string transformation the decision was trivial, as all of the strings represent a category. This mean that each string category only had to be translated to a numeric category, which is a function that the Panda's (a data treatment python library) Dataframe object offered.

As for the null values, several approaches could be used:

1.   Delete the rows which contained a null value in any column.
2.   Transform the null values to a category, because fortunately all the null values appeared in category type features.
3.   Replace the null values for the most repeated value within the patients for said feature.

The decision taken was to go for the second approach, because it meant no information was lost and no manipulation of the representation of the real world would occur.

With regards of the missing values in the target featyure, these would not be considered, as they cannot be used to train any model. However, they would later be used as a demonstration of the trained models.

# Functions used

The following section contains all the defined functions used across the notebook.

## Configuration
"""

#Imports
import pandas as pd
from sklearn import svm
from pandas import read_csv
import matplotlib.pyplot as plt
import numpy as np
import warnings
from sklearn import neighbors
from sklearn.model_selection import train_test_split, KFold, cross_val_score
import seaborn as sns
from scipy import stats
from sklearn.decomposition import PCA
from sklearn import preprocessing
from collections import Counter
from datetime import datetime
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.tree import export_graphviz

# Warnings config
warnings.filterwarnings('ignore')

"""## Data treatment. 

In order:

*   convertSimple: converts the CSV file that is passed to a dataframe
*   convertWithCategories: converts the dataframe so that all columns specified have their values converted to categories. Each category represents a number.
*   convertDeleteNulls: converts the dataframe so that every row that has at least one column with a null value is deleted from the dataframe.
*   convertDeleteColumn: deletes a complete column from a dataframe.
*   deleteRowsWithOutlyingValues: deletes all outlying values 
*   getAllColumnsButID: returns an array of all the columns of a dataframe, except the ID.
*   transformPCA: transforms a dataframe into a reduced dimentionality version, based on the number of components passed.
*   pcaTransformer: transform a dataset to PCA and visualizes correlation between resulting components.


"""

def convertSimple(filename):
    data = pd.read_csv("{}.csv".format(filename))
    return data

def convertWithCategories(data, columnNames):
    data_copy = data.copy()
    for column in columnNames:
        # Sex
        if column == 'Sex':
          data_copy['Sex'].replace(to_replace="Woman", value=0, inplace=True)
          data_copy['Sex'].replace(to_replace="Man", value=1, inplace=True)
        # Smoking
        if column == 'Smoking':
          data_copy['Smoking'].replace(to_replace="Previous smoker", value=1, inplace=True)
          data_copy['Smoking'].replace(to_replace="Smoker", value=2, inplace=True)
          data_copy['Smoking'].replace(to_replace="No smoker", value=0, inplace=True)

        # pathologies
        if column == 'Pathologies':
          data_copy['Pathologies'].replace(to_replace="No", value=0, inplace=True)
          data_copy['Pathologies'].replace(to_replace="Yes", value=1, inplace=True)
          data_copy['Pathologies'].replace(to_replace=np.nan, value=-1, inplace=True)

        # Pregnancy
        if column == 'Pregnant':
          data_copy['Pregnant'].replace(to_replace="No", value=0, inplace=True)
          data_copy['Pregnant'].replace(to_replace="Yes", value=1, inplace=True)
          data_copy['Pregnant'].replace(to_replace=np.nan, value=-1, inplace=True)

        # Food_Sugar
        if column == 'Food_Sugar':
          data_copy['Food_Sugar'].replace(to_replace="Never", value=0, inplace=True)
          data_copy['Food_Sugar'].replace(to_replace="Sometimes", value=1, inplace=True)
          data_copy['Food_Sugar'].replace(to_replace="Several times a week", value=2, inplace=True)
          data_copy['Food_Sugar'].replace(to_replace="Once a day", value=3, inplace=True)
          data_copy['Food_Sugar'].replace(to_replace="Several times a day", value=4, inplace=True)
          data_copy['Food_Sugar'].replace(to_replace=np.nan, value=-1, inplace=True)

        # Fat_Salty
        if column == 'Fat_Salty':
          data_copy['Fat_Salty'].replace(to_replace="Never", value=0, inplace=True)
          data_copy['Fat_Salty'].replace(to_replace="Sometimes", value=1, inplace=True)
          data_copy['Fat_Salty'].replace(to_replace="Several times a week", value=2, inplace=True)
          data_copy['Fat_Salty'].replace(to_replace="Once a day", value=3, inplace=True)
          data_copy['Fat_Salty'].replace(to_replace="Several times a day", value=4, inplace=True)
          data_copy['Fat_Salty'].replace(to_replace=np.nan, value=-1, inplace=True)

        # Soda
        if column == 'Soda':
          data_copy['Soda'].replace(to_replace="Never", value=0, inplace=True)
          data_copy['Soda'].replace(to_replace="Sometimes", value=1, inplace=True)
          data_copy['Soda'].replace(to_replace="Several times a week", value=2, inplace=True)
          data_copy['Soda'].replace(to_replace="Once a day", value=3, inplace=True)
          data_copy['Soda'].replace(to_replace="Several times a day", value=4, inplace=True)
          data_copy['Soda'].replace(to_replace=np.nan, value=-1, inplace=True)

        # Alcohol
        if column == 'Alcohol':
          data_copy['Alcohol'].replace(to_replace="Never", value=0, inplace=True)
          data_copy['Alcohol'].replace(to_replace="Sometimes", value=1, inplace=True)
          data_copy['Alcohol'].replace(to_replace="Several times a week", value=2, inplace=True)
          data_copy['Alcohol'].replace(to_replace="Once a day", value=3, inplace=True)
          data_copy['Alcohol'].replace(to_replace="Several times a day", value=4, inplace=True)
          data_copy['Alcohol'].replace(to_replace=np.nan, value=-1, inplace=True)

        # Frequence_Appoint_Dentist
        if column == 'Frequence_Appoint_Dentist':
          data_copy['Frequence_Appoint_Dentist'].replace(to_replace="Never", value=0, inplace=True)
          data_copy['Frequence_Appoint_Dentist'].replace(to_replace="Once a year", value=1, inplace=True)
          data_copy['Frequence_Appoint_Dentist'].replace(to_replace="2-3 a year", value=2, inplace=True)
          data_copy['Frequence_Appoint_Dentist'].replace(to_replace="Regularly", value=3, inplace=True)
          data_copy['Frequence_Appoint_Dentist'].replace(to_replace=np.nan, value=-1, inplace=True)

        # Hygiene_Dental
        if column == 'Hygiene_Dental':
          data_copy['Hygiene_Dental'].replace(to_replace=np.nan, value=-1, inplace=True)

        # Gingivorrhagia
        if column == 'Gingivorrhagia':
          data_copy['Gingivorrhagia'].replace(to_replace="Absent", value=0, inplace=True)
          data_copy['Gingivorrhagia'].replace(to_replace="Provoked", value=1, inplace=True)
          data_copy['Gingivorrhagia'].replace(to_replace="Spontanées", value=2, inplace=True)
          data_copy['Gingivorrhagia'].replace(to_replace="Corrélées (cycle, etc…)", value=3, inplace=True)
          data_copy['Gingivorrhagia'].replace(to_replace=np.nan, value=-1, inplace=True)

        # Stress_Daily
        if column == 'Stress_Daily':
          data_copy['Stress_Daily'].replace(to_replace=np.nan, value=-1, inplace=True)

        # PI
        if column == 'PI':
          data_copy['PI'].replace(to_replace=np.nan, value=-1, inplace=True)

        # Diagnosis
        if column == 'Diagnosis':
          data_copy['Diagnosis'].replace(to_replace="Gingivitis", value=0, inplace=True)
          data_copy['Diagnosis'].replace(to_replace="Periodontitis", value=1, inplace=True)
          data_copy['Diagnosis'].replace(to_replace="Healthy", value=2, inplace=True)
          data_copy['Diagnosis'].replace(to_replace=np.nan, value=-1, inplace=True)
    
    return data_copy

def convertDeleteNulls(data, columnNames):
    data_copy = data.copy()
    # Delete null values
    data_copy = data_copy.dropna(axis=0, subset=columnNames)
    
    return data_copy

def convertDeleteColumn(data, columnName):
    data_copy = data.copy()
    del data_copy[columnName]

    return data_copy

def deleteRowsWithOutlyingValues(dataframe, columns):
    data_copy = dataframe.copy()
    # For each column, first it computes the Z-score of each value in the column, relative to the column mean and standard deviation.
    # Then is takes the absolute of Z-score because the direction does not matter, only if it is below the threshold.
    # Finally, result of this condition is used to index the dataframe.
    return data_copy[(np.abs(stats.zscore(dataframe[columns])) < 3.5)]

def getAllColumnsButID(dataFrame):
    columns = []
    for col in dataFrame.columns:
        if col != 'ID':
            columns.append(col)
    return columns

def transformPCA(dataframe, numberOfComponents):
    pca = PCA(n_components=numberOfComponents)
    pca.fit(dataframe)
    return pca.transform(dataframe)

#PCA + correlation heatmap function here
def pcaTransformer(dataframe, percentage, ids, y_label, DoMap, ReturnNeeded) :
  dataframe = pd.DataFrame(transformPCA(dataframe, percentage))
  dataframe = pd.DataFrame(np.hstack([ids, dataframe, y_label]))
  dataframe = dataframe.rename(columns={0:'ID', len(dataframe.columns)-1:'Diagnosis'})
  if DoMap :
    a = [nb for nb in range(len(dataframe.columns)-1) if nb!=0]
    a.append('Diagnosis')
    plotCorrelationHeatmap(dataframe, a)
  if ReturnNeeded :
    return dataframe

"""## Visualization

In order:


*   plotColumns: plots all the columns in the dataframe in a scatter plot with the x axis being the ID of the row.
*   printNullColumns: prints all the columns that have at least one null value in any of the rows of the dataset.
*   plotCorrelationHeatmap: plots a correlation heatmap between the columns of a data frame, based on the columns that want to be analyzed.


"""

def plotColumns(dataFrame):
    for column in dataFrame.columns:
      if column == "ID":
        continue
      number = len(dataFrame[column].unique())
      plt.hist(dataFrame[column], bins=number, color='steelblue', edgecolor='black', linewidth=1.0)
      plt.xlabel(column)
      plt.ylabel("Number of occurrences")
      plt.show()

def printNullColumns(dataframe):
    columns = dataframe.columns

    columnsNull = []

    for col in columns:
        for index, row in dataframe.iterrows():
            if pd.isna(row[col]):
                columnsNull.append(col)
                break
    print("Columns with null values: ", columnsNull)

def plotCorrelationHeatmap(dataframe, columnsToPlot):
    correlation = dataframe[columnsToPlot].corr()
    #set size of the plot
    plt.figure(figsize = (16,5))
    # plot the heatmap and annotation on it
    heatmap = sns.heatmap(correlation, xticklabels=correlation.columns,
                          yticklabels=correlation.columns, annot=True)
    return heatmap

"""## Model Training"""

def svm_scores(dataframe, kernel_name):
  print(kernel_name)
  first = datetime.now() #intialize timer
  i = dataframe.copy()
  clf = svm.SVC(kernel=kernel_name)
  y = i.iloc[:,-1] #take only the last column
  x = i.drop(i.iloc[:,[0,-1]],axis=1) #drop the first and the last column          

  LVOOfolds = KFold(n_splits=x.shape[0]-1)
  tenKfolds = KFold(n_splits=10)
  scores = cross_val_score(clf, x, y_label.to_numpy().ravel(), cv=LVOOfolds)
  scores2 = cross_val_score(clf, x, y.to_numpy().ravel(), cv=tenKfolds)
  print('LVOOCV : ')
  print("Accuracy: %0.2f" % (scores.mean()))
  print('10 folds : ')
  print("Accuracy: %0.2f" % (scores2.mean()))

  #Calculating spent time on each dataframe
  finish = datetime.now()
  total = finish - first
  print(total)
  print("")

def applyKNN(dataFrame, numberOfNeighbours):
  # datetime object containing current date and time
  first = datetime.now()

  # load the data
  i = dataFrame.copy()
  dataTarget = i.iloc[:,-1] #take only the last column
  dataFeatures = i.drop(i.iloc[:,[0,-1]],axis=1) #drop the first and the last column  

  # splitting
  # X_train, X_test, y_train, y_test = train_test_split(
  #     dataFeatures, dataTarget, test_size=0.20)

  # modelling
  knn = neighbors.KNeighborsClassifier(n_neighbors=numberOfNeighbours)

  #CV
  LVOOfolds = KFold(n_splits=dataFeatures.shape[0]-1)
  tenKfolds = KFold(n_splits=10)
  scores = cross_val_score(knn, dataFeatures, dataTarget.to_numpy().ravel(), cv=LVOOfolds)
  scores2 = cross_val_score(knn, dataFeatures, dataTarget.to_numpy().ravel(), cv=tenKfolds)
  print('LVOOCV : ')
  print("Accuracy: %0.2f" % (scores.mean()))
  print('10 folds : ')
  print("Accuracy: %0.2f" % (scores2.mean()))

  # datetime object containing current date and time
  finish = datetime.now()
  total = finish - first
  print(total)
  print("")

def random_forest(dataFrame, depth):
  first = datetime.now()
  
  #preparing data
  i = dataFrame.copy()
  y = i.iloc[:,-1]
  X = i.drop(i.iloc[:,[0,-1]],axis=1)
  
  #the model
  clf = RandomForestClassifier(max_depth=depth)

  #CV
  LVOOfolds = KFold(n_splits=X.shape[0]-1)
  tenKfolds = KFold(n_splits=10)
  scores = cross_val_score(clf, X, y.to_numpy().ravel(), cv=LVOOfolds)
  scores2 = cross_val_score(clf, X, y.to_numpy().ravel(), cv=tenKfolds)
  print('LVOOCV : ')
  print("Accuracy: %0.2f" % (scores.mean()))
  print('10 folds : ')
  print("Accuracy: %0.2f" % (scores2.mean()))

  finish = datetime.now()
  total = finish - first
  print(total)

"""# Data visualization and treatment

The first step to be done was to parse the data and delete the rows with null values on the diagnosis column. 

Afterwards, the data was parsed to convert every specified row to a category, so that the string values were converted to numbers.
"""

data = convertSimple('data')
dataFiltered = convertDeleteNulls(data, ['Diagnosis'])
dataFilteredCat = convertWithCategories(dataFiltered, ['Sex', 'Smoking', 'Pathologies', 'Pregnant',
                                                           'Food_Sugar', 'Fat_Salty', 'Soda', 'Alcohol', 'Frequence_Appoint_Dentist',
                                                           'Hygiene_Dental', 'Stress_Daily', 'Gingivorrhagia', 'PI', 'Diagnosis'])

"""After the basic data conversion, the data had to be inspected to see the distribution of values within the dataset. The data can be visualized below in a barplot where the X axis corresponded to the values and the Y axis to the number of times this value was repeated in the dataset."""

plotColumns(dataFilteredCat)

"""It's visible that the categorization went smoothily, as all the plots consists on clear lines on integer values. However, it can be seen in the BMI column that there is an error, with a value being way outside of the range. This can be explained by human error on inputing the value. To deal with that value, it was decided to just remove it from the dataframe, as there wass no clear way to replace that value for another one.

This removal was performed using a function that removes all outlying values from the dataframe in the columns specified. The functions searches for the Z-value of each row with regards f the column being analyzed and deletes every row that returns an absolute value of more than 3.5 in said Z-value, as it is recommended [here](https://www.itl.nist.gov/div898/handbook/eda/section3/eda35h.htm).

Note: Z-value represents, for a specific value, how many standard deviations it is from the mean.
"""

dataFilteredCatNoOutlier = deleteRowsWithOutlyingValues(dataFilteredCat, ['BMI'])
plotColumns(dataFilteredCatNoOutlier[['ID','BMI']])

"""After eliminating the outlying value, a correlation analysis between the variables was performed. This allowed to see what were the features that more influenced the final diagnosis and also allowed to observe if there were variables that provided the same type of information, so that they could be later merged or eliminated.

It needs to be considered that this correlation anaysis, which uses the pearson method, only measures linear correlation. This means that this score is not a guarantee that the variables are not correlated, however it gives a good indication on what can be filtered out so that the data is of better quality to train the models.
"""

plotCorrelationHeatmap(
        dataFilteredCatNoOutlier, getAllColumnsButID(dataFilteredCatNoOutlier))

"""It can be seen that, for the diagnosis label, there are some variables that have a correlation close to 0, which suggests that they do not influence the outcome. These are:

1.   Fat_Salty with a score of -0.017 
2.   Hygiene_Dental with a score of -0.037
3.   Sex with a score of 0.059
4.   Frequence_Appoint_Dentist with a score of 0.098

There are others, but it was decided to consider these 4 due to them having an absolute value lower than 0.1.

Another thing that can be highlighted is that there are highly correlated variables:

1.   Sex and pregnancy are highly correlated.
2.   Age and: BMI, pathologies, smoking, pregnancy and alcohol.
3.   Alcohol and smoking.
4.   BMI and Alcohol.
5. Soda and: Fat_Salty and Food_Sugar.

There are other somehow correlated values, but it was decided to focus on the variables having an arbitrary absolute value of 0.4 or more.

This information can be useful in performing a PCA analysis, as a number of main components to be used could be inferred from this information. It is also useful to explain the number of resulting components when using a percentage in the PCA conversion.

After the visualization of linear correlation between the features, PCA conversion can be performed based on the information. 

First, a PCA based on all features can be performed and, afterwards, a PCA based on the reduced feature dataframe can also be done. In a first approach, a standard ratio of explained variance of 85% was used.

For both, it was first needed to separate the features from the label, perform the PCA on the features and then reunite the new components with the corresponding label to reperform the correlation analysis.

Afterwards, a normalization on the data was done and then the PCA was performed.
"""

ids = pd.DataFrame(dataFilteredCatNoOutlier['ID'])
y_label = pd.DataFrame(dataFilteredCatNoOutlier['Diagnosis'])
x_label = dataFilteredCatNoOutlier.drop(['ID', 'Diagnosis'], axis=1)
#We normalize the data, so that PCA gets better results
x_label = dataFilteredCatNoOutlier.values #returns a numpy array
min_max_scaler = preprocessing.MinMaxScaler()
x_label_scaled = pd.DataFrame(min_max_scaler.fit_transform(x_label))

#fit pca with 85% explained variance
pca_all_85 = pcaTransformer(x_label_scaled, 0.85, ids, y_label, True, True)

"""PCA with 85% of explained variance ends up having 10 features out of the initial 15. The correlation between variables has greatly decreased and the correlation between variables and label has increased in some cases."""

dataDeletedColumns = dataFilteredCatNoOutlier.copy()
dataDeletedColumns = convertDeleteColumn(dataDeletedColumns, 'Hygiene_Dental')
dataDeletedColumns = convertDeleteColumn(dataDeletedColumns, 'Frequence_Appoint_Dentist')
dataDeletedColumns = convertDeleteColumn(dataDeletedColumns, 'Fat_Salty')
dataDeletedColumns = convertDeleteColumn(dataDeletedColumns, 'Sex')

ids = pd.DataFrame(dataDeletedColumns['ID'])
y_label = pd.DataFrame(dataDeletedColumns['Diagnosis'])
x_label = dataDeletedColumns.drop(['ID', 'Diagnosis'], axis=1)

#We normalize the data, so that PCA gets better results
x_label = dataDeletedColumns.values
min_max_scaler = preprocessing.MinMaxScaler()
x_label_scaled = pd.DataFrame(min_max_scaler.fit_transform(x_label))

#fit pca with 85% explained variance
pca_deleted_columns_85 = pcaTransformer(x_label_scaled, 0.85, ids, y_label, True, True)

"""PCA with 85% of explained variance and deleting the features: (fat_salty, sex, hygiene_dental, Frequence_Appoint_Dentist) ends up having 8 features out of the initial 10. The correlation between variables has again greatly decreased and the correlation between variables and label has again increased.

## Dataframes to be used ahead

After performing these analysis an data treatments, 4 main datasets were derived from the original one:

1.   Data without null values, all features converted to categories, no outlier value in BMI.
2.   Data without null values, all features converted to categories, no outlier value in BMI, without columns that, according to the correlation study, seem to not influence the diagnosis.
3. PCA 85% done to the first dataset.
4. PCA 85% done to the second dataset.
"""

# We end up with the following sets of data:

# Data without null values, all features converted to categories, no outlier value in BMI
print(dataFilteredCatNoOutlier)

# Same as before, but without columns that, according to the correlation study, seem to not influence the diagnosis
print(dataDeletedColumns)

# PCA done the (so, PCA 85% to the first dataset)
print(pca_all_85)

# PCA done to relevant variables (so, PCA 100% to the second dataset)
print(pca_deleted_columns_85)

# Creating a list for simply importing the 4 dataframes together, anywhere we want.

four_dataframes = [dataFilteredCatNoOutlier,dataDeletedColumns,pca_all_85,pca_deleted_columns_85]

"""# Creating, tuning and training the models

After all the data was initially visualized and preprocessed, the process to create the models begun. The chosen models were:

1.   SVM
2.   KNN
3.   Random forest

These were chosen primarily because of their classification nature.

## SVM

The first test that was to be done was to decide which dataset, and with which configuration, yielded the best results. 

The datasets were mentioned above. As for the configuration, the main parameter that was to be tested with was the kernel of the SVM model. The kernels tried were: linear, radial basis function, polynomial with a degree of 3 and sigmoid.
"""

c=1
for data in four_dataframes:
  print('DATASET ' + str(c))
  c+=1
  svm_scores(data,'linear')
  print("---")
  svm_scores(data,"rbf")
  print("---")
  svm_scores(data,"poly")
  print("---")
  svm_scores(data,"sigmoid")
  print("---")
c=1

"""As per the above observation, SVM's **linear** kernel on both PCA Dataframes got 100% Accuracy.

So knowing that SVM has a good performance on both PCA Dataframes, with a linear kernel, it was then tried to manipulate the percentage of the PCA analysis.
"""

#Doing again the custom dataset so we can iterate with the different 
#values for PCA
dataDeletedColumns = dataFilteredCatNoOutlier.copy()
dataDeletedColumns = convertDeleteColumn(dataDeletedColumns, 'Hygiene_Dental')
dataDeletedColumns = convertDeleteColumn(dataDeletedColumns, 'Frequence_Appoint_Dentist')
dataDeletedColumns = convertDeleteColumn(dataDeletedColumns, 'Fat_Salty')
dataDeletedColumns = convertDeleteColumn(dataDeletedColumns, 'Sex')

ids = pd.DataFrame(dataDeletedColumns['ID'])
y_label = pd.DataFrame(dataDeletedColumns['Diagnosis'])
x_label = dataDeletedColumns.drop(['ID', 'Diagnosis'], axis=1)

#We normalize the data, so that PCA gets better results
x_label = dataDeletedColumns.values
min_max_scaler = preprocessing.MinMaxScaler()
x_label_scaled = pd.DataFrame(min_max_scaler.fit_transform(x_label))

for i in range(19):  
  #fit pca with incrementing % of explained variance
  total = ((i+1) * 5)/100
  pca_deleted_columns = pcaTransformer(x_label_scaled, total, ids, y_label, False, True)
  print("-------- PCA with %.2f of explained variance: ---------" % total)
  svm_scores(pca_deleted_columns, 'linear')

"""As it was observed, using the dataframe with the reduced amount of features and then creating a PCA varying incrementally in 5%, the 100% results is gotten from 85% onwards.

This can be explained due to the fact of every remaining feature holding valuable information about the diagnosis and, as such, the more they are changed and merged, the worse the result gets.

## KNN

KNN was then tried. In an initial approach an arbitrary number of neighbours were chosen to be 6. With this paremeter fixed, the different datasets were tested.

Only after determining the best dataset, were the number of neighbours changed so that a best configuration could be found.
"""

for data in four_dataframes:
  applyKNN(data, 6)

"""In this case, the best results were gotten with the dataset that eliminates the unrelated features and performs PCA on it. This is the same thing that occurs on the SVM algorithm so a trend can be spotted so far.

Afterwards, the number of neighbours were then tuned.
"""

first = datetime.now()

for i in range(20):      
  print("-------- KNN with %s neighbours: ---------" % (i+1))
  applyKNN(four_dataframes[3], i+1)

finish = datetime.now()
total = finish - first
print(total)

"""The best results were had with the number of neighbours being 11, with 97% accuracy. There were others, but the smallest one was chosen due to the algorithm having to check less neighbours.

After the dataset was chosen and the number of neighbours aswell, the percentage to perform the PCA was tuned.
"""

#Doing again the custom dataset so we can iterate with the different 
#values for PCA
dataDeletedColumns = dataFilteredCatNoOutlier.copy()
dataDeletedColumns = convertDeleteColumn(dataDeletedColumns, 'Hygiene_Dental')
dataDeletedColumns = convertDeleteColumn(dataDeletedColumns, 'Frequence_Appoint_Dentist')
dataDeletedColumns = convertDeleteColumn(dataDeletedColumns, 'Fat_Salty')
dataDeletedColumns = convertDeleteColumn(dataDeletedColumns, 'Sex')

ids = pd.DataFrame(dataDeletedColumns['ID'])
y_label = pd.DataFrame(dataDeletedColumns['Diagnosis'])
x_label = dataDeletedColumns.drop(['ID', 'Diagnosis'], axis=1)

#We normalize the data, so that PCA gets better results
x_label = dataDeletedColumns.values
min_max_scaler = preprocessing.MinMaxScaler()
x_label_scaled = pd.DataFrame(min_max_scaler.fit_transform(x_label))

for i in range(19):  
  #fit pca with incrementing % of explained variance
  total = ((i+1) * 5)/100
  pca_all_KNNPCATEST = pcaTransformer(x_label_scaled, total, ids, y_label, False, True)
  print("-------- PCA with %.2f of explained variance: ---------" % total)
  applyKNN(pca_all_KNNPCATEST, 11)

"""The best configuration according to the accuracy is gotten with a PCA of 85%, 50% or 45%. This means that, for the KNN algorithm and its determination of distance, some of the variables may be given the same information as other ones, so a merge of them becomes beneficial.

## Random Forest

The third and final algorithm to be tested is the random forest. This algorithms creates a set of different decision trees and, for each input, decides the class for the output based on the most decided upon class by all the trees.

As before, firstly the dataframe to be chosen was decided through testing of each of the available ones. For the forest to work equally in each dataset, the deepness of the trees was decided to be set at an arbitrary value of 20.
"""

random_forest(four_dataframes[0], 20)
random_forest(four_dataframes[1], 20)
random_forest(four_dataframes[2], 20)
random_forest(four_dataframes[3], 20)

"""As it can be seen, again the best result was gotten by the last dataset.

After this was decided, the PCA performed was to be decided, since only with a fixed feature number the depth of the tree could be tuned.

The PCA analysis was done, as before, using the arbitrary depth of 20.
"""

#Doing again the custom dataset so we can iterate with the different 
#values for PCA
dataDeletedColumns = dataFilteredCatNoOutlier.copy()
dataDeletedColumns = convertDeleteColumn(dataDeletedColumns, 'Hygiene_Dental')
dataDeletedColumns = convertDeleteColumn(dataDeletedColumns, 'Frequence_Appoint_Dentist')
dataDeletedColumns = convertDeleteColumn(dataDeletedColumns, 'Fat_Salty')
dataDeletedColumns = convertDeleteColumn(dataDeletedColumns, 'Sex')

ids = pd.DataFrame(dataDeletedColumns['ID'])
y_label = pd.DataFrame(dataDeletedColumns['Diagnosis'])
x_label = dataDeletedColumns.drop(['ID', 'Diagnosis'], axis=1)

#We normalize the data, so that PCA gets better results
x_label = dataDeletedColumns.values
min_max_scaler = preprocessing.MinMaxScaler()
x_label_scaled = pd.DataFrame(min_max_scaler.fit_transform(x_label))



for i in range(19):
  total = ((i+1) * 5)/100
  pca_all_RFPCATEST = pcaTransformer(x_label_scaled, total, ids, y_label, False, True)
  print("-------- PCA with %.2f of explained variance: ---------" % total)
  random_forest(pca_all_RFPCATEST, 20)

"""The best results are gotten with the PCA configured to 85%. Again, this might have been due to the fact of some variables not giving additional information for the trees to work optimally.

Afterwards, the depth was tested to observe if any other depth yields a better result. Depth was tested from 5 to 50.
"""

#Doing again the custom dataset so we can iterate with the different 
#values for PCA
dataDeletedColumns = dataFilteredCatNoOutlier.copy()
dataDeletedColumns = convertDeleteColumn(dataDeletedColumns, 'Hygiene_Dental')
dataDeletedColumns = convertDeleteColumn(dataDeletedColumns, 'Frequence_Appoint_Dentist')
dataDeletedColumns = convertDeleteColumn(dataDeletedColumns, 'Fat_Salty')
dataDeletedColumns = convertDeleteColumn(dataDeletedColumns, 'Sex')

ids = pd.DataFrame(dataDeletedColumns['ID'])
y_label = pd.DataFrame(dataDeletedColumns['Diagnosis'])
x_label = dataDeletedColumns.drop(['ID', 'Diagnosis'], axis=1)

#We normalize the data, so that PCA gets better results
x_label = dataDeletedColumns.values
min_max_scaler = preprocessing.MinMaxScaler()
x_label_scaled = pd.DataFrame(min_max_scaler.fit_transform(x_label))

pca_all_RFPCATEST = pcaTransformer(x_label_scaled, 0.85, ids, y_label, False, True)

for i in range(10):
  total = (i + 1) * 5
  print("-------- Random forest with %s depth: ---------" % total)
  random_forest(pca_all_RFPCATEST, total)

"""As per observed, the best result was gotten with a depth of 25.

# Prediction of missing data

## Datasets without PCA (1 and 2)

The chosen algorithm to perform the predictions was SVM linear as it was the model that yielded the best results during the train/test stage.
"""

dfff = convertWithCategories(convertSimple('data'), ['Sex', 'Smoking', 'Pathologies', 'Pregnant',
                                                           'Food_Sugar', 'Fat_Salty', 'Soda', 'Alcohol', 'Frequence_Appoint_Dentist',
                                                           'Hygiene_Dental', 'Stress_Daily', 'Gingivorrhagia', 'PI', 'Diagnosis'])
deleteRowsWithOutlyingValues(dfff, ['BMI'])

#Preparing sets to predict whithout PCA

#keeping only unpredicted lines
diagnosis = dfff.iloc[:,-1]
for i in range(len(diagnosis)):
  if diagnosis[i] != -1:
    dfff.drop(i, inplace=True)
dfff.drop(columns=['Diagnosis', 'ID'], inplace=True)

#15 features, all unpredicted lines
allFeatures = dfff.copy()

#15-4=10 features
customDataSet = dfff.copy()
customDataSet = convertDeleteColumn(customDataSet, 'Hygiene_Dental')
customDataSet = convertDeleteColumn(customDataSet, 'Frequence_Appoint_Dentist')
customDataSet = convertDeleteColumn(customDataSet, 'Fat_Salty')
customDataSet = convertDeleteColumn(customDataSet, 'Sex')

Tdataframe = four_dataframes[0].copy()
clf = svm.SVC(kernel='linear')
y = Tdataframe.iloc[:,-1] #take only the last column
x = Tdataframe.drop(Tdataframe.iloc[:,[0,-1]],axis=1) #drop the first and the last column          
clf.fit(x,y)
print(clf.predict(allFeatures))

Tdataframe = four_dataframes[1].copy()
clf = svm.SVC(kernel='linear')
y = Tdataframe.iloc[:,-1] #take only the last column
x = Tdataframe.drop(Tdataframe.iloc[:,[0,-1]],axis=1) #drop the first and the last column      
clf.fit(x,y)
print(clf.predict(customDataSet))

"""Reminder :
"Gingivitis": 0
"Periodontitis": 1
"Healthy": 2

There's only 6 differences between the predictions based on the 1st and the 2nd dataset. The total number of predicted lines is 49.

---

## Datasets with PCA (datasets 3 and 4)

Finally, the values were predicted with the best datasets.
"""

PCADATA = convertWithCategories(convertSimple('data'), ['Sex', 'Smoking', 'Pathologies', 'Pregnant',
                                                           'Food_Sugar', 'Fat_Salty', 'Soda', 'Alcohol', 'Frequence_Appoint_Dentist',
                                                           'Hygiene_Dental', 'Stress_Daily', 'Gingivorrhagia', 'PI', 'Diagnosis'])
deleteRowsWithOutlyingValues(PCADATA, ['BMI'])

#Preparing sets to predict whith PCA

#Isolating unpredicted lines
diagnosis = PCADATA.iloc[:,-1]
for i in range(len(diagnosis)):
  if diagnosis[i] != -1:
    PCADATA.drop(i, inplace=True)

#15 features + PCA
ids = pd.DataFrame(PCADATA['ID'])
y_label = pd.DataFrame(PCADATA['Diagnosis'])
x_labels = PCADATA.drop(['ID', 'Diagnosis'], axis=1)

#We normalize the data, so that PCA gets better results
min_max_scaler = preprocessing.MinMaxScaler()
x_labels_scaled = pd.DataFrame(min_max_scaler.fit_transform(x_labels.values))

#fit pca with the same number of columns as the model dataset
PCADATA_ALL = pcaTransformer(x_labels_scaled, 10, ids, y_label, False, True).drop(['ID', 'Diagnosis'], axis=1)

#10 features + PCA
customDataSet = PCADATA.copy()
customDataSet = convertDeleteColumn(customDataSet, 'Hygiene_Dental')
customDataSet = convertDeleteColumn(customDataSet, 'Frequence_Appoint_Dentist')
customDataSet = convertDeleteColumn(customDataSet, 'Fat_Salty')
customDataSet = convertDeleteColumn(customDataSet, 'Sex')

ids = pd.DataFrame(customDataSet['ID'])
y_label = pd.DataFrame(customDataSet['Diagnosis'])
x_label = customDataSet.drop(['ID', 'Diagnosis'], axis=1)

#We normalize the data, so that PCA gets better results
x_label_scaled = pd.DataFrame(min_max_scaler.fit_transform(x_label.values))

#fit pca with the same number of columns as the model dataset
PCADATA_DELETED = pcaTransformer(x_label_scaled, 8, ids, y_label, False, True).drop(['ID', 'Diagnosis'], axis=1)

Tdataframe = four_dataframes[2].copy()
clf = svm.SVC(kernel='linear')
y = Tdataframe.iloc[:,-1] #take only the last column
x = Tdataframe.drop(Tdataframe.iloc[:,[0,-1]],axis=1) #drop the first and the last column          
clf.fit(x,y)
print(clf.predict(PCADATA_ALL))

Tdataframe = four_dataframes[3].copy()
clf = svm.SVC(kernel='linear')
y = Tdataframe.iloc[:,-1] #take only the last column
x = Tdataframe.drop(Tdataframe.iloc[:,[0,-1]],axis=1) #drop the first and the last column          
clf.fit(x,y)
print(clf.predict(PCADATA_DELETED))

"""There's 12 differences between the predictions based on the 3rd and the 4th dataset. The total number of predicted lines is 49.
1/4 of the predicted lines are different between the two, which is suprising given the fact that both of these datasets gave 100% accuracy using **cross validation**, and both of them are using SVM linear kernel.

Reminder :
"Gingivitis": 0
"Periodontitis": 1
"Healthy": 2

So we decided to check if there is any error with the predictions on the DATASET 4, by comparing the predicted values of the dataset and the actual values of the label :
"""

data_copy = four_dataframes[3].copy()
y = data_copy.iloc[:,-1] #take only the last column
x = data_copy.drop(data_copy.iloc[:,[0,-1]],axis=1) #drop the first and the last column
data_copy = data_copy.drop(data_copy.iloc[:,[0,-1]],axis=1)
clf = svm.SVC(kernel='linear')
clf.fit(x,y)
prediction = clf.predict(data_copy)
y_list = y.to_numpy().tolist()

c=0
for i in range(len(prediction)):
  if (prediction[i] != y_list[i]):
    c +=1
print(c)

"""As we can see here, there's no difference between the labels and the predictions so there's no error in the model.
"""